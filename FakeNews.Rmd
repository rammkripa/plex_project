---
title: 'Faux & Friends: A Fake News Detection and Analysis Service'
author: "Allen Chen, Ram Mukund Kripa"
date: "8/4/2020"
output: html_document
runtime: shiny
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

### Packages used in this Project

```{r packages, include = FALSE}
library(tidyverse)
library(hoaxy)
library(rtweet)
library(tidytext)
library(here)
library(shiny)
```

## Part 1: Creating the Fake News Detection Engine

### Creating the Training Dataset

#### Obtaining some Real News

```{r real}
real_news <- get_timelines(c("reuters","bbcworld"),
                           n = 1000)
real_corona <- real_news %>%
  mutate(text = tolower(text)) %>%
  filter(str_detect(string = text, 
                    pattern = str_c("covid","corona","pandemic","virus",sep = "|")))
```

#### Obtaining some Fake News

```{r fake}
fake_news_topics = c("plandemic","pandemic","5G","coronavirus","covid")

articles_list <- list()
for (topic in fake_news_topics){
  articles_list[[topic]] <- hx_articles(q = topic,
                                        sort_by = c("relevant"))
}

fake_articles_corona <- bind_rows(articles_list,.id = "topic") %>%
  distinct(canonical_url,.keep_all = TRUE) %>%
  filter(site_type == "claim",score>120)

fake_tweets_corona  <- hx_tweets(ids = unique(fake_articles_corona$id))

fake_corona <- fake_tweets_corona %>%
  group_by(id) %>%
  top_n(n = 7, wt = tweet_id)

fake_covid <- lookup_tweets(fake_corona$tweet_id)

head(fake_covid)

```

#### Merging real and fake news

```{r merging}
train_list <- list()
train_list[["Real"]] <- real_corona
train_list[["Fake"]] <- fake_covid

training_day <- bind_rows(train_list,.id = "class") %>%
  select(class,screen_name,status_id,text,source,hashtags)

```

### Cleaning

```{r}
remove_ats <- function(stringy){
  first_part <- stringr::str_remove(stringy,"@(.*)")
  second_part <- stringr::str_remove(stringy,"(.*)@")
  second_part <- stringr::str_remove(second_part,".*? ")
  return(stringr::str_c(first_part,second_part,sep = " "))
}

train_text_df <- training_day %>%
  select(status_id,class,text) %>%
  rowwise() %>%
  mutate(text = remove_ats(text)) %>%
  mutate(text = sub("http.*", "", text) ) %>%
  mutate(text = sub("amp.*", "", text) ) %>%
  filter(text!="",text!=" ")%>%
  unnest_tokens(output = "tweet_words",
                input = text,
                token = "words") %>%
  drop_na(tweet_words) %>%
  filter(!str_detect(string = tweet_words,
                    pattern = "[0-9]")) %>%
  anti_join(stop_words, by = c("tweet_words"="word")) %>%
  mutate(tweet_words = SnowballC::wordStem(tweet_words))

```

### What are the most common words in Real and Fake News?

```{r}

train_text_df %>%
  mutate(tweet_words = iconv(tweet_words, from = "latin1", to = "ASCII")) %>%
  filter(!is.na(tweet_words)) %>%
  group_by(class,tweet_words) %>%
  summarize(count = n()) %>%
  top_n(n = 10,wt = count) %>%
  ggplot(mapping = aes(x=reorder_within(tweet_words,count,class),
                       y = count, fill = class))+
  geom_col()+
  scale_x_reordered()+
  facet_wrap(~class,scales="free")+
  coord_flip()+
  labs(x = "Word Stems",
       y = "Count",
       title = "Most Common words in Fake & Real News",
       fill = "Type of News")

```

Note that no model has been applied to this training data yet!
What you see here is based only on the raw count of words in real and fake tweets.

### Applying Term Frequency - Inverse Document Frequency

```{r}

train_text_df %>%
  mutate(tweet_words = iconv(tweet_words, from = "latin1", to = "ASCII")) %>%
  filter(!is.na(tweet_words)) %>%
  #ggplot(mapping = aes(x = class))+
  #geom_bar()
  group_by(class,tweet_words) %>%
  summarize(count = n()) %>%
  bind_tf_idf(term = tweet_words,
              document = class,
              n = count) %>%
  group_by(class) %>%
  top_n(n = 6, wt = tf_idf) %>%
  ggplot(mapping = aes(x=reorder_within(tweet_words,tf_idf,class),
                       y = tf_idf, fill = class))+
  geom_col()+
  scale_x_reordered()+
  facet_wrap(~class,scales="free_y")+
  coord_flip()+
  labs(x = "Word Stems",
       y = "TF-IDF Score",
       title = "Word Stems in Fake & Real News with Highest TFIDF",
       fill = "Type of News")
  

```


### Creating the Document Term Matrix

```{r dtm}

```

### Training the Model

### Words that make news Fake

## Part 2: Interactive Environment

Is your friend spreading COVID misinformation?
Find out now!

### Please Enter a Twitter Handle

```{r input_handle, echo=FALSE, eval = FALSE}
inputPanel(
  textInput("input_name", label = "Twitter Username",
              value = "realDonaldTrump")
)

```

### Most Common Words in Recent Tweets

```{r plot, eval = FALSE}
renderPlot({
 
  user_dat <- get_timeline(user = input$input_name,
                           n = 3200
                           )
  user_clean <- user_dat %>%
    mutate(text = sub("http.*", "", text) ) %>%
    unnest_tokens(output = "tweet_words",
                  input = text,
                  token = "words") %>%
    select(screen_name,created_at,tweet_words) %>%
    anti_join(stop_words, by = c("tweet_words" = "word")) %>%
    filter(tweet_words!="amp",
           tweet_words!=tolower(input$input_name))
    #%>%
    #mutate(tweet_words = SnowballC::wordStem(tweet_words))
  
  user_clean %>%
    group_by(tweet_words) %>%
    summarize(count = n()) %>%
    arrange(-count) %>%
    head(n = 6L) %>%
    ggplot(mapping = aes(x = reorder(tweet_words,count),
                         y = count))+
      geom_col()+
      coord_flip()+
      labs(title = glue::glue("Most Common Words for ",input$input_name),
           x = "Word Stem",
           y = "Number of Occurences")
  
})
```