---
title: 'Faux & Friends: A Fake News Detection and Analysis Service'
author: "Allen Chen, Ram Mukund Kripa"
date: "8/4/2020"
output: html_document
runtime: shiny
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

### Packages used in this Project

```{r packages}
library(tidyverse)
library(hoaxy)
library(rtweet)
library(tidytext)
library(here)
library(shiny)
```

## Part 1: Creating the Fake News Detection Engine

### Creating the Training Dataset

```{r realnews}

real_news <- get_timelines(c("reuters","bbcworld"))
```

```{r fake}
fake_news_topics = c("pizzagate","plandemic","chemtrails","george soros","5G","Bush did 911","anti vax")

articles_list <- list()
for (topic in fake_news_topics){
  articles_list[[topic]] <- hx_articles(q = topic)
}

articles_df <- bind_rows(articles_list,.id = "topic") %>%
  head(n=300L)
tweets_df <- hx_tweets(ids = unique(articles_df$id))
relevant_tweets <- tweets_df %>%
  group_by(id) %>%
  summarize(title = max(title),
            tweet_id = max(tweet_id))

rel_tweet_df <- lookup_tweets(relevant_tweets$tweet_id)

```

### Cleaning

```{r}
remove_ats <- function(stringy){
  first_part <- stringr::str_remove(stringy,"@(.*)")
  second_part <- stringr::str_remove(stringy,"(.*)@")
  second_part <- stringr::str_remove(second_part,".*? ")
  return(stringr::str_c(first_part,second_part,sep = " "))
}

tweet_text_df <- rel_tweet_df %>%
  rowwise() %>%
  mutate(text = remove_ats(text)) %>%
  mutate(text = sub("http.*", "", text) ) %>%
  filter(text!="",text!=" ")%>%
  unnest_tokens(output = "tweet_words",
                input = text,
                token = "words") %>%
  select(user_id,status_id,screen_name,created_at,source,tweet_words) %>%
  anti_join(stop_words, by = c("tweet_words"="word")) %>%
  
  mutate(tweet_words = SnowballC::wordStem(tweet_words))

```

### Cleaning Real news

```{r}
real_news_terms <- real_news %>%
  rowwise() %>%
  mutate(text = remove_ats(text)) %>%
  mutate(text = sub("http.*", "", text) ) %>%
  filter(text!="",text!=" ")%>%
  unnest_tokens(output = "tweet_words",
                input = text,
                token = "words") %>%
  select(user_id,status_id,screen_name,created_at,source,tweet_words) %>%
  anti_join(stop_words, by = c("tweet_words"="word")) %>%
  
  mutate(tweet_words = SnowballC::wordStem(tweet_words))

```

Is your friend spreading COVID misinformation?
Find out now!

```{r}

tweet_text_df %>%
  group_by(tweet_words) %>%
  summarize(count = n()) %>%
  arrange(-count) %>%
  head(n = 10L) %>%
  ggplot(mapping = aes(x=reorder(tweet_words,count),
                       y = count))+
  geom_col()+
  coord_flip()+
  labs(x = "Word Stems",
       y = "Count",
       title = "Most Common words in Fake News")

```

### Binding

```{r}
blist <- list()
blist[["fake news"]] = tweet_text_df
blist[["real news"]] = real_news_terms
training_day <- bind_rows(blist,.id = "Type")
```

### Term Frequency in Real and Fake News

```{r}

training_day %>%
  group_by(tweet_words,Type) %>%
  summarize(count = n()) %>%
  group_by(Type) %>%
  top_n(n = 10, wt = count) %>%
  ggplot(mapping = aes(x = tweet_words, y = count))+
  geom_col()+
  coord_flip()+
  facet_wrap(~Type,scales = "free")

```


### Creating the Document Term Matrix

```{r dtm}
text_tfidf %>%
cast_dtm(document = )
```

### Training the Model

### Words that make news Fake

## Part 2: Interactive Environment

### Please Enter a Twitter Handle

```{r input_handle, echo=FALSE, eval = FALSE}
inputPanel(
  textInput("input_name", label = "Twitter Username",
              value = "realDonaldTrump")
)

```

### Most Common Words in Recent Tweets

```{r plot, eval = FALSE}
renderPlot({
 
  user_dat <- get_timeline(user = input$input_name,
                           n = 3200
                           )
  user_clean <- user_dat %>%
    mutate(text = sub("http.*", "", text) ) %>%
    unnest_tokens(output = "tweet_words",
                  input = text,
                  token = "words") %>%
    select(screen_name,created_at,tweet_words) %>%
    anti_join(stop_words, by = c("tweet_words" = "word")) %>%
    filter(tweet_words!="amp",
           tweet_words!=tolower(input$input_name))
    #%>%
    #mutate(tweet_words = SnowballC::wordStem(tweet_words))
  
  user_clean %>%
    group_by(tweet_words) %>%
    summarize(count = n()) %>%
    arrange(-count) %>%
    head(n = 6L) %>%
    ggplot(mapping = aes(x = reorder(tweet_words,count),
                         y = count))+
      geom_col()+
      coord_flip()+
      labs(title = glue::glue("Most Common Words for ",input$input_name),
           x = "Word Stem",
           y = "Number of Occurences")
  
})
```