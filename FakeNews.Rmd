---
title: 'Faux & Friends: A Fake News Detection and Analysis Service'
author: "Allen Chen, Ram Mukund Kripa"
date: "8/4/2020"
output: html_document
runtime: shiny
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

### Packages used in this Project

```{r packages}
library(tidyverse)
library(hoaxy)
library(rtweet)
library(tidytext)
library(here)
library(shiny)
```

## Part 1: Creating the Fake News Detection Engine

### Creating the Training Dataset

```{r}
fake_news_topics = c("pizzagate","flat earth","aliens")

articles_list <- list()
for (topic in fake_news_topics){
  articles_list[[topic]] <- hx_articles(q = topic)
}

articles_df <- bind_rows(articles_list,.id = "topic") 
tweets_df <- hx_tweets(ids = unique(articles_df$id))
relevant_tweets <- tweets_df %>%
  group_by(id) %>%
  summarize(title = max(title),
            tweet_id = max(tweet_id))

rel_tweet_df <- lookup_tweets(relevant_tweets$tweet_id)

```

### Cleaning

### Most common Words in Fake and Real News

```{r}

rel_tweet_df %>%
  mutate(text = sub("http.*", "", text) ) %>%
  unnest_tokens(output = "tweet_words",
                input = text,
                token = "words") %>%
  anti_join(stop_words, by = c("tweet_words"="word")) %>%
  mutate(tweet_words = SnowballC::wordStem(tweet_words)) %>%
  group_by(tweet_words) %>%
  summarize(count = n()) %>%
  arrange(-count) %>%
  head(n = 10L) %>%
  ggplot(mapping = aes(x=reorder(tweet_words,count),
                       y = count))+
  geom_col()+
  coord_flip()+
  labs(x = "Word Stems",
       y = "Count",
       title = "Most Common words in Fake News")

```

### Creating the Document Term Matrix

### Training the Model

### Words that make news Fake

## Part 2: Interactive Environment

### Please Enter a Twitter Handle

```{r input_handle, echo=FALSE, eval = FALSE}
inputPanel(
  textInput("input_name", label = "Twitter Username",
              value = "realDonaldTrump")
)

```

### Most Common Words in Recent Tweets

```{r plot, eval = FALSE}
renderPlot({
 
  user_dat <- get_timeline(user = input$input_name,
                           n = 3200
                           )
  user_clean <- user_dat %>%
    mutate(text = sub("http.*", "", text) ) %>%
    unnest_tokens(output = "tweet_words",
                  input = text,
                  token = "words") %>%
    select(screen_name,created_at,tweet_words) %>%
    anti_join(stop_words, by = c("tweet_words" = "word")) %>%
    filter(tweet_words!="amp",
           tweet_words!=tolower(input$input_name))
    #%>%
    #mutate(tweet_words = SnowballC::wordStem(tweet_words))
  
  user_clean %>%
    group_by(tweet_words) %>%
    summarize(count = n()) %>%
    arrange(-count) %>%
    head(n = 6L) %>%
    ggplot(mapping = aes(x = reorder(tweet_words,count),
                         y = count))+
      geom_col()+
      coord_flip()+
      labs(title = glue::glue("Most Common Words for ",input$input_name),
           x = "Word Stem",
           y = "Number of Occurences")
  
})
```