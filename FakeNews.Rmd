---
title: 'Faux & Friends: A Fake News Detection and Analysis Service'
author: "Allen Chen, Ram Mukund Kripa"
date: "8/4/2020"
output: html_document
runtime: shiny
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r packages, include = FALSE}
library(tidyverse)
library(hoaxy)
library(rtweet)
library(tidytext)
library(here)
library(shiny)
library(text2vec)
```

```{r perc}
api_key <- "r4UWPcaTY9JIa4w4Qr9lygsvR"
api_secret_key <- "XYZ5j2ZFzTASaVGJjEi98DKg2pjhlRTqm4xervTlZToAKAvWko"
access_token <- "3234588499-awSgTAungXRlOH5nrxGoqQWzdE0Q6VEM3HKwJSO"
access_token_secret <- "umeZS3B6Qj1SQuWkWZWOHQJji2xEukew9pAgvZ8CfsB38"

## store authentication information
token <- create_token(
  app = "CFSS test 2",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret
)

```

```{r vectormodel, include = FALSE}

training_path <-  here("training_wheels.csv")
training_set <- read_csv(training_path)

token_train <- itoken(training_set$text,
                     preprocessor = tolower,
                     tokenizer = word_tokenizer,
                     ids = training_set$unique_id)
t_vocab <- create_vocabulary(token_train)
t_vectorizer <- vocab_vectorizer(t_vocab)
dtm_train <- create_dtm(token_train, t_vectorizer)
dim(dtm_train)

library(glmnet)
NFOLDS = 4
glmnet_classifier = cv.glmnet(x = dtm_train, y = training_set[['Class']], 
                              family = 'binomial', 
                              # L1 penalty
                              alpha = 1,
                              # interested in the area under ROC curve
                              type.measure = "auc",
                              # 5-fold cross-validation
                              nfolds = NFOLDS,
                              # high value is less accurate, but has faster training
                              thresh = 1e-3,
                              # again lower number of iterations for faster training
                              maxit = 1e3)

plot(glmnet_classifier)


```



Is your friend spreading COVID misinformation?

***Find out now!***

## Please Enter a Twitter Handle

```{r input_handle}
inputPanel(
  textInput("input_name", label = "Twitter Username",
              value = "realDonaldTrump"),
  sliderInput(inputId = "num_recent", label = "Look at most Recent ____ Tweets", min = 200, max = 3000, value = 1200, step = 20)
)

```

### Most Common Words in Recent Tweets

```{r plot}

renderPlot({
  user_dat <- get_timeline(user = input$input_name,
                           n = input$num_recent
                           )
  user_clean <- user_dat %>%
    mutate(text = sub("http.*", "", text) ) %>%
    unnest_tokens(output = "tweet_words",
                  input = text,
                  token = "words") %>%
    select(screen_name,created_at,tweet_words) %>%
    anti_join(stop_words, by = c("tweet_words" = "word")) %>%
    filter(tweet_words!="amp",
           tweet_words!=tolower(input$input_name))
    #%>%
    #mutate(tweet_words = SnowballC::wordStem(tweet_words))
  
 p1 <-  user_clean %>%
    group_by(tweet_words) %>%
    summarize(count = n()) %>%
    arrange(-count) %>%
    head(n = 6L) %>%
    ggplot(mapping = aes(x = reorder(tweet_words,count),
                         y = count))+
      geom_col()+
      coord_flip()+
      labs(title = glue::glue("Most Common Words for ",input$input_name),
           x = "Word Stem",
           y = "Number of Occurences")
 
  library(ggwordcloud)
 library(patchwork)
 love_words
 p2 <- user_clean %>%
    group_by(tweet_words) %>%
    summarize(count = n()) %>%
   top_n(n = 20, wt = count) %>%
   ggplot(mapping = aes(label = tweet_words,size = count))+
   geom_text_wordcloud()
 
 p1/p2
   
  
})
```

Here, we see the Most Common words used by this user. These alone, however, do not tell us anything about the "Fake-ness" or "Real-ness" of this user's news. 

### Does this Account post Fake News?

We obtained a training dataset using the Hoaxy and Twitter APIs. Then, we used the Text2Vec Package in R to vectorize each token and trained a GLMNet Neural network model. This model, when applied to our Test Data, tells us the probability of a tweet spreading Fake News related to COVID 19.  

```{r testnetmod}
renderPlot({
  user_dat <- get_timeline(user = input$input_name,
                           n = input$num_recent
                           )
test_data <- user_dat
test_set <- test_data %>%
  mutate(text = tolower(text)) %>%
  filter(str_detect(string = text,
                    pattern = str_c("covid","corona","china","virus","pandemic","fauci","cdc",sep="|")
                    )) %>%
  select(status_id,text) %>%
  rename("unique_id"="status_id") %>%
  mutate(text = sub("http.*", "", text) )

it_test <- word_tokenizer(test_set$text) %>%
itoken(ids = test_set$unique_id, 
                 # turn off progressbar because it won't look nice in rmd
                 progressbar = FALSE)

dtm_test <- create_dtm(it_test, t_vectorizer)

preds <- predict(glmnet_classifier, dtm_test, type = 'response')

test_result <- ifelse(test = any(preds<0.5),
       yes = "Fake",
       no = "Real")

prediction_tbl <- as_tibble(preds) %>%
  mutate(tweet_no = seq(from = 1, to = length(preds), by = 1)) %>%
  mutate(Class = ifelse(`1`>0.5,"Real","Fake")) %>%
  mutate(Class = as_factor(Class)) 

p1 <- prediction_tbl %>%
  ggplot(mapping = aes(x = tweet_no, y = `1`,color = Class))+
  geom_point()+
  geom_hline(yintercept = 0.5)+
  labs(x = "Tweet", y = "Probability of being Real News",
       title = glue::glue("COVID Tweets of ",input$input_name," Classified"))
p2 <- prediction_tbl %>%
  ggplot(mapping = aes(x = Class, fill = Class))+
  geom_bar(stat = "count")+
  labs(x = "Type of News",
       y = "Count",
       title = "Relative Amount of Fake News")

library(patchwork)

p1+p2

})

```

### Is this Account Fake News?

The second plot asks a slightly different question. It aims to find out whether the account itself primarily posts real or fake COVID news.

